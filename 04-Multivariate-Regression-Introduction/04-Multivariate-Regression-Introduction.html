<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multivariate Regression: Introduction and Ceteris Paribus</title>
    <meta charset="utf-8" />
    <meta name="author" content="Justin Kirkpatrick" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="../../EC420_S21.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Multivariate Regression: Introduction and Ceteris Paribus
## EC420 MSU
### Justin Kirkpatrick
### Last updated January 18, 2021

---



layout: true

&lt;div class="msu-header"&gt;&lt;/div&gt; 


---
class: inverseMSU
name: Overview

# This lecture  

__Goal:__

1. Introduce two-variable (multivariate) regression

2. Motivate use of multivariate regression

3. Relate concepts from single variable to multivariate

4. Refine concept of *ceteris paribus*

5. Concept of "partialing out"

6. Extend multivariate from two to `\(K\)` variables

7. Specification errors
  - Irrelevant variables and omitted variables




---
class: heading-slide

What is multivariate regression?

---
class: MSU
# Multivariate regression

### Multivariate regression is the estimation of the PRF:
`$$E[Y | X_1, X_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2$$`



### Where we previously had PRF:

`$$E[Y|X] = \beta_0 + \beta_1 x$$`



----

### The SRF for two variables is:

`$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2$$`



### We still have one error term, `\(u\)`

`$$y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + u_i$$`
### And we estimate `\(\hat{\beta} = \{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2 \}\)` the same way.


---
class: MSU

# Multivariate regression

### Examples
- We want to explain country-level *life expectancy* as a function of *gdp per capita* and *population growth*.
`$$LifeExp_i = \beta_0 + \beta_1 gdppc_i + \beta_2 popgrowth_i + u_i$$`


- We want to explain *mortality rate* with *number of cigarettes smoked* and *average daily caloric intake*
`$$Mortality_i = \beta_0 + \beta_1 cigarettes_i + \beta_2 calories_i + u_i$$`

- We want to explain *wage* with *education* and *ability*:
`$$Wage_i = \beta_0 + \beta_1 educ_i + \beta_2 ability_i + u_i$$`
---
class:MSU
# Multivariate regression


## Ceteris Paribus - *all else held equal*
`$$Wage_i = \beta_0 + \beta_1 educ_i + \beta_2 ability_i + u_i$$`


We interpret `\(\beta_1\)` as "the effect of `\(educ\)` on the expectation of `\(wage\)`, *all else held equal*"

What other random variables are we holding equal:
- `\(ability\)`
--

- `\(u\)` too!

## This means:
`\(\beta_1 = \frac{\Delta Wage}{\Delta educ}\)` when `\(\underbrace{\frac{\Delta Wage}{\Delta ability} = \frac{\Delta Wage}{\Delta u} = 0}_{\text{all else held equal}}\)`

---
class: MSU
# Multivariate regression

## And a similar interpretation for `\(\beta_2\)`
We interpret `\(\beta_2\)` as "the effect of `\(experience\)` on the expectation of `\(wage\)`, *all else held equal*"


## This means:
`\(\beta_2 = \frac{\Delta Wage}{\Delta ability}\)` when `\(\underbrace{\frac{\Delta Wage}{\Delta educ} = \frac{\Delta Wage}{\Delta u} = 0}_{\text{all else held equal}}\)`

----

--

Does this require that `\(\frac{\Delta educ}{\Delta experience}\)` be zero?

Nope. But we are measuring the effect of one *while holding the other equal to zero*.

---
class: MSU
# Multivariate regression

## We can interpret `\(\beta_0\)` as:
`\(\beta_0 = E[wage]\)` when `\(educ\)` and `\(ability\)` **and** `\(u=0\)`

### This is because:
`$$E[wage|educ,ability] = \beta_0 + \beta_1 educ + \beta_2 ability$$`
or, in general notation

`$$E[Y|x_1, x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2$$`

A slightly different interpretation, and unique to the `\(\beta_0\)`.

---
class: MSU
# Multivariate regression

### What if we *should* use two variables, but we only use one?

We could run the regression `\(wage_i = \beta_0 + \beta_1 educ_i + u_i\)`, but we probably think `\(ability_i\)` also affects wages.

What if we don't observe `\(ability_i\)`?
- Just because we don't observe it doesn't mean it isn't affecting `\(wage_i\)`
- It *is present in the error term*. 
- Let's make a new variable called `\(\tilde{u} = \delta_1 ability_i + u_i\)`

`$$wage_i = \beta_0 + \beta_1 educ_i + \underbrace{\delta_1 ability_i + u_i}_{\tilde{u}_i}$$`

  - Note: usually the `\(\sim\)` over a coefficient or variable (like `\(\tilde{u}\)`) will indicate it is related to, but different from, the non- `\(\sim\)` version.

  - `\(\delta_1\)` is just the effect of `\(ability\)` on `\(wage\)`



---
class: MSU
# Multivariate regression

### We can naively write this as a single variable regression:

`$$wage_i = \beta_0 + \beta_1 educ_i + \tilde{u}_i$$`

--

## But wait!

- We think `\(E[ability|educ]&gt;0\)`

  - Then this violates the assumption that `\(E[\tilde{u}|X] = 0\)`

Because 

1. `\(\frac{\Delta \tilde{u}}{\Delta ability} = \delta_1 \neq 0\)`

2. `\(\frac{\Delta ability}{\Delta educ} \neq 0\)`

`\(\Rightarrow \frac{\Delta \tilde{u}}{\Delta educ} \neq 0\)`

---
class: MSU
# Multivariate regression

### Bias

Recall that we could show `\(E[\hat{\beta}_1]=\beta_1\)` if and only if `\(E[u|X]=0\)`.

Looking at `\(\tilde{u}_i = \delta_1 ability_i + u_i\)`, we can see why `\(E[\tilde{u}|educ]\neq 0\)`

- Thus, `\(\beta_1\)` in the single-variable regression was **biased**.



---
class: MSU
# Multivariate regression

#### Adding in `\(ability\)` as a second variable fixes this:
`$$wage_i = \beta_0 + \beta_1 educ_i + \beta_2 ability_i + u_i$$`

because `\(u_i\)` does not change with `\(educ\)` or `\(ability\)`.

Now, `\(E[u|X_1, X_2] = E[u|educ, ability] = 0\)`

---
class: MSU
# Multivariate regression

## Multivariate regression allows us to account for the effect of both `\(ability\)` and `\(educ\)`

`$$wage = \beta_0 + \beta_1 educ + \beta_2 ability + u$$`

And we can calculate the change in `\(wage\)` from any change in `\(ability\)` and `\(educ\)` using the SRF:

`$$\Delta\widehat{wage} = \widehat{\beta_1} \Delta educ + \widehat{\beta_2} \Delta ability$$`

---
class: MSU
# Multivariate regression

## This also means our assumption is now:

`$$E[u|educ, wage] = 0$$`

## Which we write in general as:

`$$E[u|x_1, x_2]=0$$`


Which is to say that we think we've got everything that could potentially be correlated with `\(x_1\)` and `\(x_2\)` out of the error term.


--

Since we want to work with some sample data (wage2.dta from Wooldridge), let's replace `\(ability\)` with `\(experience\)`, which is in the dataset....


---
class: MSU
# Multivariate regression

When we estimate `\(wage_i = \beta_0 + \beta_1 educ_i + \beta_2 experience_i + u_i\)`, we are fitting a *plane*:

&lt;img src="04-Multivariate-Regression-Introduction_files/figure-html/LoadWage-1.png" width="70%" /&gt;

---
class: MSU
# Multivariate regression

When we estimate `\(wage_i = \beta_0 + \beta_1 educ_i + \beta_2 experience_i + u_i\)`, we are fitting a *plane*:

&lt;img src="04-Multivariate-Regression-Introduction_files/figure-html/LoadWageXX-1.png" width="70%" /&gt;
---
class: MSU
# Multivariate regression

The best fit is no longer a line, but a *plane* with a slope in the `\(educ\)` and the `\(exper\)` axis of `\(\beta_1\)` and `\(\beta_2\)`

Fitted values from a regression on the data in the previous slide where `\(\beta_{educ} = 76.22\)` and `\(\beta_{exper} = 17.64\)`.

&lt;img src="04-Multivariate-Regression-Introduction_files/figure-html/Wage2-1.png" width="60%" style="display: block; margin: auto;" /&gt;




---
class: MSU
# Multivariate regression

## How do we estimate `\(\hat{\beta}\)`?

Remember our two assumptions that let us derive `\(\hat{\beta}_1\)` and `\(\hat{\beta}_0\)`:
`$$E[u] = 0,\quad E[u|x] = 0$$`

--

Now, we want to estimate `\(\hat{\beta} = \{ \hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2\}\)`
- And we have two `\(x\)`'s: `\(x_1\)` and `\(x_2\)`.

`$$E[u] = 0, \quad E[u|x_1] = 0, \quad E[u|x_2] = 0$$`

We have **three** moment conditions, and three unknowns to estimate. We can do that!

---
class: MSU
# Multivariate regression

## These three moment conditions give us the following to start with:
`$$E[y_i - \beta_0 - \beta_1 x_1 - \beta_1 x_2] = E[u]= 0$$`
`$$E[x_1(y_i - \beta_0 - \beta_1 x_1 - \beta_1 x_2)] = E[x_1 u] = 0$$`
`$$E[x_2(y_i - \beta_0 - \beta_1 x_1 - \beta_1 x_2)] = E[x_2 u] = 0$$`

---

class: MSU
# Multivariate regression

### But don't worry, we won't derive them directly from this, but that's how we would do it.

---
class: MSU
# Multivariate regression

### Let's talk notation for a second:

- I will use `\(\beta\)` as the coefficients we are estimating ( `\(\hat{\beta}\)` )
- When talking about the right hand side (the covariates), I'll either call them `\(x_1\)`, `\(x_2, \cdots\)`
  - Or sometimes just using the variable names: `\(wage = \beta_0 + \beta_1 educ\)`
  - Or sometimes with subscripts: `\(y = \beta_0 + \beta_{educ} x_{educ} + u\)`
  
- And sometimes, if we want to emphasize that two regressions are wholly different, I will use `\(\delta\)` or `\(\gamma\)` instead of `\(\beta\)`

- `\(u\)` and `\(v\)` will represent errors in two different regressions
---
class: MSU
# Multivariate regression

## Partialing out
Imagine if we had two `\(x\)`'s, `\(x_{temp}\)` and `\(x_{rain}\)` that both had an effect on `\(y\)`, but were closely related.

We could look at `\(\beta_{temp}\)` in
`$$y = \beta_0 + \beta_{temp} x_{temp} + u$$`

And `\(\beta_{rain}\)` in 
`$$y = \beta_0 + \beta_{rain} x_{rain} + u$$`

We learned from the previous section that `\(\beta_{temp}\)` is biased when `\(x_{rain}\)` is in the error term `\(u\)`, and vice versa.

That is, `\(\beta_{temp}\)` is going to "pick up some of the effect" of `\(x_{rain}\)`.

---
class: MSU
# Multivariate regression

## Partialing out
So, would `\(\tilde{\beta}_{temp}\)` in the following (correct) specification equal `\(\beta_{temp}\)` from the previous slide?
`$$y = \tilde{\beta}_0 + \tilde{\beta}_{temp} x_{temp} + \tilde{\beta}_{rain} x_{rain} + \tilde{u}$$`
--

## No!

Once we include both variables, we will get a different estimate for `\(\tilde{\beta}\)` than before since each effect is isolated (*ceteris paribus*) 

So, to calculate the correct `\(\tilde{\beta}_{temp}\)`, we need estimates that take the effect of `\(\tilde{\beta}_{rain}\)` into consideration.

This is called **partialing out**.

---
class: MSU
# Multivariate regression

## One way we can estimate unbiased `\(\beta_{temp}\)` is the following way:

First, estimate the regression of `\(x_{temp}\)` on `\(x_{rain}\)`
`$$x_{temp} = \delta_0 + \delta_{rain} x_{rain} + v$$`

Couple of things:
- `\(x_{temp}\)` is on the left hand side. 
- **We are "explaining temperature with rainfall"**
- Using `\(\delta\)` to show that these are different coefficients

--

That error term, `\(v\)` has an interpretation
- `\(v\)` is the `\(temp\)` that is **not explained by** `\(rain\)`
- `\(\delta_{rain}x_{rain}\)` is the `\(temp\)` that **is** explained by `\(rain\)`

#### `\(v\)` is `\(temp\)` that has had `\(rain\)` "partialed out"

---
class: MSU
# Multivariate regression

## Of course, we have a sample analog for `\(v\)`, the SRF residuals:
`$$\hat{v} = x_{temp} - (\hat{\delta}_0 + \hat{\delta}_{rain} x_{rain})$$`

Remember, `\(v\)` still varies along with `\(x_{temp}\)`, but it is not correlated at *all* with `\(x_{rain}\)`.

---
class: MSU
# Multivariate regression

### Now, if we want to get the correct value for `\(\beta_{temp}\)` in the full regression:
`$$y = \gamma_0 + \gamma_1 \hat{v} + u$$`

- We do not use `\(x_{rain}\)` directly.

- We use `\(\hat{v}\)` and leave `\(x_{rain}\)` out. 

- `\(\hat{v}\)` is correlated with `\(x_{temp}\)`, but not with `\(x_{rain}\)`

- Put another way, `\(\hat{v}\)` contains only the part of `\(x_{temp}\)` that is not correlated with `\(x_{rain}\)`.

## One can show that `\(\gamma_1 = \tilde{\beta}_{temp}\)`

---
class: MSU
# Multivariate regression

## One can show that `\(\gamma_1 = \tilde{\beta}_{temp}\)`, the unbiased estimate.

That is, we get the (unbiased) coefficient one would get from regressing 

`$$y = \tilde{\beta}_0 + \tilde{\beta}_{temp} x_{temp} + \tilde{\beta}_{rain} x_{rain} + \tilde{u}$$` 

by first "partialing" `\(x_{rain}\)` out of `\(x_{temp}\)` then regressing what is left on `\(y\)`.



---
class: MSU
# Multivariate regression

Similarly, one can do the same for `\(x_{rain}\)`:
`$$x_{rain} = \kappa_0 + \kappa_1 x_{temp} + w$$`

Then use the residuals, `\(\hat{w}\)`, in a regression:
`$$y = \alpha_0 + \alpha_1 \hat{w} + \epsilon$$`

And `\(\tilde{\beta}_{rain} = \alpha_1\)`

--

Since `\(\hat{\tilde{\beta}}_{rain} = \hat{\alpha}_1 = \frac{Cov(y,\hat{w})}{Var(\hat{w})}\)`, we can say that `\(\beta_{rain}\)` is the effect of `\(x_{rain}\)` *once we've taken out the effect of* `\(x_{temp}\)` and vice versa.

---
class: MSU
# Multivariate regression

Since we get the same `\(\hat{\beta}_1\)` if we
- Partial out the effect of `\(x_2\)` and run a single variable regression, or
- Run a two-variable regression

Then we can think of the `\(\tilde{\beta}_1\)` in:

`$$y = \tilde{\beta}_0 + \tilde{\beta}_1 x_1 + \tilde{\beta}_2 x_2 + \tilde{u}$$`
As the effect of `\(x_1\)` on `\(y\)` **after partialing `\(x_2\)` out of `\(x_1\)`**, and vice versa.

### Multivariate regression automatically partials out each of the `\(x's\)`.
---
class: MSU
# Multivariate regression

## Let's compare a simple and multiple regression estimates
- `\(y = \tilde{\beta}_0 + \tilde{\beta}_1 x_1 + \tilde{\beta}_2 x_2 + \tilde{u}\)`
- `\(y = \beta_0 + \beta_1 x_1 + u\)`

How will `\(\tilde{\beta}_1\)` differ from `\(\beta_1\)`?

--

It depends on the relationship between `\(x_2\)` and `\(x_1\)`
`$$x_2 = \delta_0 + \delta_1 x_1 + v$$`

If we take `\(x_2 = \delta_0 + \delta_1 x_1 + v\)` and substitute it into the first equation above:
`$$y = \tilde{\beta}_0 + \tilde{\beta}_1 x_1 + \tilde{\beta}_2 (\delta_0 + \delta_1 x_1 + v) + \tilde{u}$$`
`$$y = \tilde{\beta}_0 + \tilde{\beta}_2 \delta_0 + \tilde{\beta}_1 x_1 + \tilde{\beta}_2 \delta_1 x_1 + \tilde{\beta}_2 v + \tilde{u}$$`
`$$y = \tilde{\tilde{\beta}}_0 + (\tilde{\beta}_1 + \tilde{\beta}_2 \delta_1 ) x_1 + \tilde{v}$$`

---
class: MSU
# Multivariate regression

### Therefore it is true that:
- `\(\hat{\beta}_1 = \hat{\tilde{\beta}}_1 + \hat{\tilde{\beta}}_2 \hat{\delta}_1\)`
  - In words: to whatever extent `\(x_1\)` and `\(x_2\)` are correlated ( `\(\delta_1\)` ), our naive `\(\hat{\beta}_1\)` will include that correlation. 

### Knowing this, when would the simple regression estimate `\(\hat{\beta}_1\)` **equal** the multiple regression (multivariate) estimate `\(\hat{\tilde{\beta}}_1\)`?

---
class: MSU
# Multivariate regression

## Will this hold empirically?
Will `\(\hat{\beta}_1\)` change when you add in `\(\hat{\beta}_2\)`?


.pull-left[
&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Model 1 &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; Model 2 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 146.952 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -272.528 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; (77.715) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; (107.263) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; educ &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 60.214 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 76.216 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; (5.695) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; (6.297) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; exper &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 17.638 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; (3.162) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Num.Obs. &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 935 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 935 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; R2 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.107 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.136 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; R2 Adj. &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.106 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.134 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AIC &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13776.9 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13748.2 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; BIC &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13791.4 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13767.6 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Log.Lik. &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -6885.458 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -6870.104 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; F &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 111.793 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 73.260 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

.pull-right[
`$$\begin{eqnarray}
wage &amp;=&amp; \beta_0 + \beta_1 educ + u \\
wage &amp;=&amp; \tilde{\beta}_0 + \tilde{\beta}_1 educ + \tilde{\beta}_2 exper + \tilde{u}  
\end{eqnarray}$$`
]

---
class: MSU
# Multivariate regression

## Will this hold empirically?
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Std. Error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; t value &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Pr(&amp;gt;|t|) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23.78 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 30.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; educ &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.91 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.06 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -15.63 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
`$$exper = \delta_0 + \delta_1 educ + u$$`
and 
`$$60.21 = 76.22 + (\underbrace{17.64}_{\hat{\tilde{\beta}}_2} \times \underbrace{-.91}_{\hat{\delta}_1})$$`
---
class: MSU
# Goodness of fit

## We can still use the same formula for R^2

`$$R^2 = \frac{SSE}{SST} = \frac{\sum_{i=1}^N (\hat{y}_i - \hat{\bar{y}})^2}{\sum_{i=1}^N (y_i - \bar{y})^2}$$`
This is because `\(R^2\)` only uses the fit of the whole model, determined by how well `\(\hat{y}\)` fits.


---
class: MSU
# Three or more variables

## Multiple regression is easily extended to many variables.

`$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + u$$`

And multiple variables, we can extend the partialing out in the following manner:
- `\(x_1 = \delta_0 + \beta_1 x_2 + \beta_2 x_3 + \cdots + \beta_k x_{k+1} + v\)`
  - `\(v\)` is the part of `\(x_1\)` that has had `\(x_2, x_3, \cdots\)` partialed out
- `\(y = \alpha_0 + \alpha_1 \hat{v}\)`
- `\(\beta_1\)` = `\(\alpha_1\)`

You can "partial out" multiple variables, leaving only variation that is uncorrelated with the other variables.

## OLS is easily extended from 2 to &gt;2 variables

---
class: MSU
# Specification errors

## We might be worried about two *specification errors*
- Including an irrelevant variable.
  - Suppose the "true model" is:
`$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u$$`
  - And we estimate:
`$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$$`
  - Then OLS is still an unbiased estimator, since *unbaisedness* holds regardless of the true value of the parameters, even if `\(\beta_j = 0\)` for some `\(j\)`.
  - Including an irrelevant variable will, however, impact the variance of the OLS estimator.

---
class: MSU
# Specification errors
## We might be worried about two *specification errors*

- Omitting a relevant variable.
`$$y = \tilde{\beta}_0 + \tilde{\beta}_1 x_1 + u$$`

  - We showed that `\(\tilde{\beta}_1 = \beta_1\)` only when `\(\beta_2 = 0\)` or `\(\delta_1 = 0\)`.
  - Size and direction depend on the sign and size of `\(\beta_2 \delta_1\)`, which depends on the relationship of the omitted variable and the included variable, `\(x_1\)`, and the outcome variable, `\(y\)`.
  - With multiple regressors, the sign and size may not be clear.
  - We can usually *"sign the bias"* if we
      1. have an idea of what is omitted, 
      2. have an idea of how it's correlated with `\(y\)`, and 
      3. have an idea of how it's correlated with one or more of `\(x_1, x_2, \cdots, x_k\)`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
